{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387775a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  Initialisation de l'extracteur SIREN\n",
      "üÜï Nouveau mod√®le vierge cr√©√©.\n",
      "\n",
      "üìö Cr√©ation des donn√©es d'entra√Ænement...\n",
      "‚ùå Dossier annonces_legales introuvable.\n",
      "‚ùå Impossible de cr√©er les donn√©es d'entra√Ænement.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import minibatch\n",
    "\n",
    "# === PARAM√àTRES ===\n",
    "MODEL_DIR = \"model_output\"\n",
    "DATA_DIR = \"data_entra√Ænement\"\n",
    "TEXTS_DIR = \"annonces_legales\"\n",
    "LABEL = \"SIREN\"\n",
    "PATTERN_SIREN = r\"\\b\\d{9}\\b\"  # Un SIREN = 9 chiffres exactement\n",
    "\n",
    "class SirenExtractor:\n",
    "    def __init__(self):\n",
    "        self.model_dir = MODEL_DIR\n",
    "        self.data_dir = DATA_DIR\n",
    "        self.texts_dir = TEXTS_DIR\n",
    "        self.label = LABEL\n",
    "        self.pattern = PATTERN_SIREN\n",
    "        self.nlp = None\n",
    "    \n",
    "    def load_or_create_model(self):\n",
    "        \"\"\"Charge un mod√®le existant ou cr√©e un nouveau mod√®le vierge\"\"\"\n",
    "        model_path = Path(self.model_dir)\n",
    "        meta_path = model_path / \"meta.json\"\n",
    "        if model_path.exists() and meta_path.exists():\n",
    "            print(\"üîÅ Mod√®le existant charg√©.\")\n",
    "            self.nlp = spacy.load(self.model_dir)\n",
    "        else:\n",
    "            print(\"üÜï Nouveau mod√®le vierge cr√©√©.\")\n",
    "            self.nlp = spacy.blank(\"fr\")\n",
    "        return self.nlp\n",
    "    \n",
    "    def extract_siren_positions(self, text):\n",
    "        \"\"\"Extrait les positions des num√©ros SIREN dans le texte\"\"\"\n",
    "        return [(m.start(), m.end()) for m in re.finditer(self.pattern, text)]\n",
    "    \n",
    "    def validate_siren(self, siren):\n",
    "        \"\"\"Validation basique du num√©ro SIREN (peut √™tre √©tendue)\"\"\"\n",
    "        if len(siren) != 9 or not siren.isdigit():\n",
    "            return False\n",
    "        # Ici vous pourriez ajouter l'algorithme de validation Luhn si n√©cessaire\n",
    "        return True\n",
    "    \n",
    "    def create_training_data(self, output_path):\n",
    "        \"\"\"Convertit les fichiers .txt en donn√©es d'entra√Ænement JSONL\"\"\"\n",
    "        if not os.path.exists(self.texts_dir):\n",
    "            print(f\"‚ùå Dossier {self.texts_dir} introuvable.\")\n",
    "            return False\n",
    "        \n",
    "        training_data = []\n",
    "        processed_files = 0\n",
    "        \n",
    "        for filename in os.listdir(self.texts_dir):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                filepath = os.path.join(self.texts_dir, filename)\n",
    "                try:\n",
    "                    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                        text = f.read().strip()\n",
    "                    \n",
    "                    if not text:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extraire les positions des SIREN\n",
    "                    entities = self.extract_siren_positions(text)\n",
    "                    \n",
    "                    # Filtrer les SIREN valides\n",
    "                    valid_entities = []\n",
    "                    for start, end in entities:\n",
    "                        siren = text[start:end]\n",
    "                        if self.validate_siren(siren):\n",
    "                            valid_entities.append((start, end, self.label))\n",
    "                    \n",
    "                    if valid_entities:  # Seulement si on a trouv√© des SIREN valides\n",
    "                        training_data.append({\n",
    "                            \"text\": text,\n",
    "                            \"entities\": valid_entities,\n",
    "                            \"source\": filename\n",
    "                        })\n",
    "                        processed_files += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Erreur lors du traitement de {filename}: {e}\")\n",
    "        \n",
    "        # Sauvegarder en JSONL\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for record in training_data:\n",
    "                json.dump(record, f, ensure_ascii=False)\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ {len(training_data)} √©chantillons cr√©√©s √† partir de {processed_files} fichiers\")\n",
    "        print(f\"‚úÖ Donn√©es sauvegard√©es dans {output_path}\")\n",
    "        return len(training_data) > 0\n",
    "    \n",
    "    def convert_to_docbin(self, jsonl_path):\n",
    "        \"\"\"Convertit les donn√©es JSONL en DocBin pour spaCy\"\"\"\n",
    "        if not os.path.exists(jsonl_path):\n",
    "            print(f\"‚ùå Fichier {jsonl_path} introuvable.\")\n",
    "            return None\n",
    "        \n",
    "        db = DocBin()\n",
    "        examples_count = 0\n",
    "        \n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    record = json.loads(line.strip())\n",
    "                    text = record[\"text\"]\n",
    "                    entities = record[\"entities\"]\n",
    "                    \n",
    "                    # Cr√©er le document\n",
    "                    doc = self.nlp.make_doc(text)\n",
    "                    \n",
    "                    # Cr√©er les entit√©s\n",
    "                    ents = []\n",
    "                    for start, end, label in entities:\n",
    "                        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "                        if span is not None:\n",
    "                            ents.append(span)\n",
    "                        else:\n",
    "                            print(f\"‚ö†Ô∏è  Entit√© ignor√©e ligne {line_num}: '{text[start:end]}'\")\n",
    "                    \n",
    "                    doc.ents = ents\n",
    "                    db.add(doc)\n",
    "                    examples_count += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Erreur ligne {line_num}: {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ {examples_count} exemples convertis en DocBin\")\n",
    "        return db\n",
    "    \n",
    "    def train_model(self, docbin, n_iter=20, batch_size=4, dropout=0.3):\n",
    "        \"\"\"Entra√Æne le mod√®le NER avec les donn√©es fournies\"\"\"\n",
    "        if not docbin:\n",
    "            print(\"‚ùå Pas de donn√©es d'entra√Ænement.\")\n",
    "            return False\n",
    "        \n",
    "        # Configurer le pipeline NER\n",
    "        if \"ner\" not in self.nlp.pipe_names:\n",
    "            ner = self.nlp.add_pipe(\"ner\", last=True)\n",
    "        else:\n",
    "            ner = self.nlp.get_pipe(\"ner\")\n",
    "        \n",
    "        # Ajouter le label\n",
    "        ner.add_label(self.label)\n",
    "        \n",
    "        # Pr√©parer les donn√©es d'entra√Ænement\n",
    "        train_examples = []\n",
    "        docs = list(docbin.get_docs(self.nlp.vocab))\n",
    "        \n",
    "        for doc in docs:\n",
    "            example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})\n",
    "            train_examples.append(example)\n",
    "        \n",
    "        # Initialiser le mod√®le\n",
    "        self.nlp.initialize(lambda: train_examples)\n",
    "        \n",
    "        print(f\"üöÄ D√©but de l'entra√Ænement ({n_iter} √©poques, {len(train_examples)} exemples)\")\n",
    "        \n",
    "        # Entra√Ænement\n",
    "        for epoch in range(n_iter):\n",
    "            random.shuffle(train_examples)\n",
    "            losses = {}\n",
    "            \n",
    "            # Entra√Æner par batch\n",
    "            batches = minibatch(train_examples, size=batch_size)\n",
    "            for batch in batches:\n",
    "                self.nlp.update(batch, drop=dropout, losses=losses)\n",
    "            \n",
    "            # Afficher les progr√®s\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"üìä √âpoque {epoch + 1:2d}/{n_iter} - Perte NER: {losses.get('ner', 0):.4f}\")\n",
    "        \n",
    "        # Sauvegarder le mod√®le\n",
    "        self.nlp.to_disk(self.model_dir)\n",
    "        print(f\"üíæ Mod√®le sauvegard√© dans {self.model_dir}\")\n",
    "        return True\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Pr√©dit les entit√©s SIREN dans un texte\"\"\"\n",
    "        if not self.nlp:\n",
    "            print(\"‚ùå Mod√®le non charg√©.\")\n",
    "            return []\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        predictions = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == self.label:\n",
    "                predictions.append({\n",
    "                    \"text\": ent.text,\n",
    "                    \"start\": ent.start_char,\n",
    "                    \"end\": ent.end_char,\n",
    "                    \"confidence\": getattr(ent, 'confidence', 'N/A')\n",
    "                })\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate_model(self, test_text_samples=None):\n",
    "        \"\"\"√âvalue les performances du mod√®le sur des √©chantillons de test\"\"\"\n",
    "        if not test_text_samples:\n",
    "            test_text_samples = [\n",
    "                \"La soci√©t√© DUPONT SARL est enregistr√©e sous le SIREN 123456789 √† Paris.\",\n",
    "                \"Les entreprises 987654321 et 111222333 ont fusionn√©.\",\n",
    "                \"SIREN: 555666777 - Soci√©t√© MARTIN & FILS\",\n",
    "                \"Aucun num√©ro SIREN dans ce texte.\",\n",
    "                \"Contact: 01.23.45.67.89 - SIREN 444555666\"\n",
    "            ]\n",
    "        \n",
    "        print(\"üîç √âvaluation du mod√®le:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, text in enumerate(test_text_samples, 1):\n",
    "            predictions = self.predict(text)\n",
    "            print(f\"Test {i}: {text[:50]}...\")\n",
    "            if predictions:\n",
    "                for pred in predictions:\n",
    "                    print(f\"  ‚úÖ SIREN trouv√©: {pred['text']} (pos: {pred['start']}-{pred['end']})\")\n",
    "            else:\n",
    "                print(\"  ‚ùå Aucun SIREN d√©tect√©\")\n",
    "            print()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    print(\"üèóÔ∏è  Initialisation de l'extracteur SIREN\")\n",
    "    extractor = SirenExtractor()\n",
    "    \n",
    "    # Cr√©er les dossiers n√©cessaires\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    # Charger ou cr√©er le mod√®le\n",
    "    extractor.load_or_create_model()\n",
    "    \n",
    "    # √âtape 1: Cr√©er les donn√©es d'entra√Ænement\n",
    "    jsonl_path = os.path.join(DATA_DIR, \"train.jsonl\")\n",
    "    print(\"\\nüìö Cr√©ation des donn√©es d'entra√Ænement...\")\n",
    "    if not extractor.create_training_data(jsonl_path):\n",
    "        print(\"‚ùå Impossible de cr√©er les donn√©es d'entra√Ænement.\")\n",
    "        return\n",
    "    \n",
    "    # √âtape 2: Convertir en DocBin\n",
    "    print(\"\\nüîÑ Conversion en format spaCy...\")\n",
    "    docbin = extractor.convert_to_docbin(jsonl_path)\n",
    "    if not docbin:\n",
    "        return\n",
    "    \n",
    "    # √âtape 3: Entra√Æner le mod√®le\n",
    "    print(\"\\nüéØ Entra√Ænement du mod√®le...\")\n",
    "    if extractor.train_model(docbin, n_iter=20):\n",
    "        print(\"\\n‚úÖ Entra√Ænement termin√© avec succ√®s!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå √âchec de l'entra√Ænement.\")\n",
    "        return\n",
    "    \n",
    "    # √âtape 4: √âvaluer le mod√®le\n",
    "    print(\"\\nüß™ √âvaluation du mod√®le...\")\n",
    "    extractor.evaluate_model()\n",
    "    \n",
    "    print(\"\\nüéâ Pipeline termin√©!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
