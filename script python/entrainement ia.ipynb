{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776337e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== EXTRACTION DES SIREN DEPUIS LES FICHIERS JSON ===\n",
      "Traitement de siren_extraits_20250611_113328.json...\n",
      "Traitement de siren_extraits_20250611_115313.json...\n",
      "Traitement de siren_extraits_20250611_120916.json...\n",
      "Traitement de siren_extraits_20250612_091547.json...\n",
      "Traitement de siren_extraits_20250612_092246.json...\n",
      "Traitement de siren_extraits_20250612_100830.json...\n",
      "Traitement de siren_extraits_20250612_105258.json...\n",
      "\\nTraitement termin√©:\n",
      "- 5 fichiers trait√©s\n",
      "- 468 SIREN uniques extraits\n",
      "\\nSIREN extraits: ['378657746', '901182303', '483445565', '783691728', '182489760', '927682864', '953849312', '953930252', '922239918', '515230613', '820021475', '493669683', '517680401', '818843609', '910337716', '904032646', '883918195', '842571234', '313285181', '929263515', '533675443', '798440798', '513194027', '901603555', '345310965', '504760323', '484551767', '492794490', '929987204', '808775639', '851389429', '844927905', '899243885', '811568120', '452951874', '903375954', '981784564', '898792502', '941411613', '452631363', '810102418', '891617730', '513643650', '888617651', '822894770', '977908557', '309054922', '410554513', '927959163', '335185104', '349496869', '326019775', '895293850', '404854937', '829080779', '412825937', '830589628', '921411138', '538260993', '383061801', '933867467', '398987149', '409451960', '401064365', '475680815', '448786384', '980904890', '950973669', '810973321', '513831784', '519343800', '914425822', '828609222', '538576380', '817591878', '488384751', '983913567', '948526348', '830408175', '910033265', '912046778', '890654494', '810001578', '978514099', '881348999', '351497649', '521573071', '987803962', '332714401', '938263548', '382896306', '838364909', '984141366', '908285364', '518899406', '451363451', '834236309', '985058429', '440934404', '503111767', '310472527', '531268191', '943669507', '850981564', '834183352', '803717602', '439336693', '804044873', '985028174', '983247636', '457506475', '811322338', '923122386', '822210944', '434333365', '898620174', '344260286', '537915993', '823132881', '908868334', '942463878', '902385780', '830406344', '937957850', '908604333', '933231268', '905291340', '799836697', '979252533', '883140907', '919719468', '940859515', '942914151', '383348604', '392661013', '821017696', '929425718', '984065490', '303609606', '892414863', '382914893', '943355586', '349379263', '833819972', '882512148', '316432962', '841991953', '484245055', '519205397', '503271124', '940686165', '529391708', '929004190', '894520758', '909177776', '933100174', '317587038', '828837302', '878873942', '404141525', '387992514', '917505927', '831828389', '938469400', '941325722', '909556573', '350731303', '821837374', '851914622', '887913424', '788804607', '903994580', '909224537', '812059384', '890240484', '819919333', '399403617', '789899218', '394948137', '390447548', '841174717', '982285017', '843487299', '538257205', '568801013', '524833373', '981471402', '880836838', '888988045', '847818663', '904688553', '535154223', '934610445', '791743925', '448947127', '493504278', '903390789', '953784428', '912308293', '942672536', '843845785', '822793774', '824286850', '987844271', '331773598', '812878742', '943819201', '833527278', '845011261', '685550659', '344366315', '799054937', '848890232', '832504740', '934633868', '538699778', '552054447', '887679686', '913810933', '935283390', '949867048', '513603472', '938630753', '849365291', '979241494', '832404487', '539191692', '980606800', '512983651', '750626269', '490486222', '932917560', '884297169', '394421572', '351710959', '524644457', '977920768', '750583486', '948790381', '983759903', '987775368', '984446823', '518709407', '910664721', '401812151', '453244782', '830015871', '810891499', '807497623', '852816040', '948572318', '913222246', '901626523', '853509784', '934670134', '813623626', '340899541', '487833709', '882650849', '445720899', '881109649', '914198924', '931556849', '400734448', '818467615', '383829983', '834033276', '483629036', '832948814', '939581054', '337788012', '987634326', '840991681', '970506812', '980608269', '902631647', '851827683', '853925584', '517583522', '532881398', '879023182', '892134701', '518927215', '847815826', '440838860', '941579740', '402863856', '909399370', '340571058', '939487682', '499235034', '891595753', '879356947', '488997933', '931435648', '904522349', '420296113', '908602394', '900030222', '897739173', '882183080', '305241788', '932909344', '948737259', '504543323', '981817273', '398621102', '982225856', '852357664', '901872010', '829408590', '882918956', '834584757', '403235252', '411308828', '931416200', '832318364', '849117957', '853969244', '935308783', '827636465', '424839488', '931523245', '383673779', '880960893', '451132534', '833239379', '888379062', '480042845', '378875645', '892543943', '925321309', '949692362', '938492584', '833143662', '918090010', '902108968', '790675771', '981658081', '897842670', '851006817', '803124940', '915074629', '928886175', '851361592', '330356999', '919003665', '338835812', '417493079', '538632431', '449334440', '844157735', '921489142', '672006483', '814575791', '497634030', '538261249', '982687865', '807652565', '908086358', '842488801', '499901205', '983379322', '795157924', '978170322', '470500943', '811168442', '790051460', '941374472', '388746836', '814301180', '948255070', '891748444', '488754631', '934268699', '523218766', '951408442', '882045586', '893314286', '880318662', '917421281', '485143648', '979141686', '915366520', '948673611', '878834811', '337926968', '478410475', '940286487', '947832093', '892217936', '388252306', '982033334', '979423902', '518776562', '794101972', '879982270', '820854693', '951036748', '902016955', '953078581', '839201381', '834120321', '829846864', '799158100', '893609776', '910382217', '353462989', '850165465', '433340882', '927478578', '905156865', '350503389', '507384683', '501911291', '529287641', '800731994', '893782813', '532446598', '850847955', '979368008', '881257091', '929199701', '850493123', '415116268', '909062606', '075750364', '752021386', '853765097', '881760441', '480855634', '915100507', '813322450', '508235413', '493636039', '790065841', '834405961', '931808331', '433720646', '889671509', '840026298', '814548905', '928583798', '493781082', '398730366', '797753829', '530018639', '852538461', '881094387', '531997294', '919190322', '941110306', '901306506', '898360524', '420452864', '801999574', '519118400', '481759363', '904433745', '325676583', '498406537', '949688006', '907720775', '814573333', '404146656', '448581314', '882818412', '891505059']\n",
      "\\n=== CR√âATION DU DATASET D'ENTRA√éNEMENT ===\n",
      "Dataset cr√©√© avec 6984 exemples valides\n",
      "Sauvegard√© dans: donnees_entrainement\\train_siren.spacy\n",
      "\\n=== ENTRA√éNEMENT DU MOD√àLE ===\n",
      "Mod√®le de base charg√©: fr_core_news_md\n",
      "Entra√Ænement avec 6984 exemples...\n",
      "It√©ration 0: Perte = 401.945\n",
      "It√©ration 5: Perte = 15.039\n",
      "It√©ration 10: Perte = 10.224\n",
      "It√©ration 15: Perte = 2.321\n",
      "It√©ration 20: Perte = 2.580\n",
      "It√©ration 25: Perte = 6.710\n",
      "\\nMod√®le entra√Æn√© et sauvegard√© dans: donnees_entrainement\\modele_siren\n",
      "\\n=== TEST DU MOD√àLE ===\n",
      "\\n=== TEST DU MOD√àLE ===\n",
      "\\nTexte: La soci√©t√© ACME a le SIREN 123456789\n",
      "  ‚Üí SIREN d√©tect√©: 123456789 (confiance: N/A)\n",
      "\\nTexte: RCS Paris 987654321\n",
      "  ‚Üí SIREN d√©tect√©: 987654321 (confiance: N/A)\n",
      "\\nTexte: Num√©ro SIREN : 741852963\n",
      "  ‚Üí SIREN d√©tect√©: 741852963 (confiance: N/A)\n",
      "\\nTexte: L'entreprise est immatricul√©e sous le num√©ro 456789123\n",
      "  ‚Üí SIREN d√©tect√©: 456789123 (confiance: N/A)\n",
      "\\n‚úÖ Processus termin√© avec succ√®s!\n",
      "üìÅ Mod√®le sauvegard√© dans: donnees_entrainement\\modele_siren\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class SirenTrainer:\n",
    "    \"\"\"Classe pour extraire des SIREN et entra√Æner un mod√®le spaCy\"\"\"\n",
    "    \n",
    "    def __init__(self, modele_base=\"fr_core_news_md\", dossier_donnees=\"donnees_entrainement\"):\n",
    "        self.modele_base = modele_base\n",
    "        self.dossier_donnees = Path(dossier_donnees)\n",
    "        self.dossier_donnees.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Donn√©es d'entra√Ænement de base\n",
    "        self.train_data_base = [\n",
    "            (\"La soci√©t√© X est identifi√©e par le SIREN 918090010.\", {\"entities\": [(39, 48, \"SIREN\")]}),\n",
    "            (\"SIREN : 981086762\", {\"entities\": [(8, 17, \"SIREN\")]}),\n",
    "            (\"Num√©ro SIREN 822 210 944 valide.\", {\"entities\": [(13, 24, \"SIREN\")]}),\n",
    "            (\"Identifi√©e sous le SIREN 938263548\", {\"entities\": [(25, 34, \"SIREN\")]}),\n",
    "            (\"Le SIREN est 942558057\", {\"entities\": [(13, 22, \"SIREN\")]}),\n",
    "            (\"Si√®ge social 10 rue des Crombions 62840 FLEURBAIX 881 109 649 RCS Arras\", {\"entities\": [(50, 59, \"SIREN\")]}),\n",
    "            (\"RCS Lille 123 456 789\", {\"entities\": [(10, 21, \"SIREN\")]}),\n",
    "            (\"Immatricul√©e sous le num√©ro SIREN 987654321\", {\"entities\": [(34, 43, \"SIREN\")]}),\n",
    "            (\"SIREN123456789\", {\"entities\": [(5, 14, \"SIREN\")]}),\n",
    "            (\"Entreprise immatricul√©e au RCS de Nantes sous le n¬∞ 456 789 123\", {\"entities\": [(52, 63, \"SIREN\")]}),\n",
    "            (\"Num√©ro d'identification 321654987 selon l'INSEE\", {\"entities\": [(25, 34, \"SIREN\")]}),\n",
    "            (\"Le num√©ro est : 789654123\", {\"entities\": [(16, 25, \"SIREN\")]}),\n",
    "            (\"Num√©ro de SIREN:123 123 123\", {\"entities\": [(17, 28, \"SIREN\")]}),\n",
    "            (\"N¬∞ SIREN : 741852963\", {\"entities\": [(11, 20, \"SIREN\")]}),\n",
    "        ]\n",
    "        \n",
    "        # SIREN suppl√©mentaires pour g√©n√©rer plus de donn√©es\n",
    "        self.liste_siren_base = [\"123456789\", \"741852963\", \"987654321\"]\n",
    "    \n",
    "    def valider_siren(self, siren_str):\n",
    "        \"\"\"Valider un num√©ro SIREN avec l'algorithme de Luhn\"\"\"\n",
    "        siren = re.sub(r'\\D', '', str(siren_str))\n",
    "        if len(siren) != 9 or not siren.isdigit():\n",
    "            return False\n",
    "        \n",
    "        total = 0\n",
    "        for i, digit in enumerate(siren):\n",
    "            n = int(digit)\n",
    "            if i % 2 == 1:\n",
    "                n *= 2\n",
    "                if n > 9:\n",
    "                    n -= 9\n",
    "            total += n\n",
    "        \n",
    "        return total % 10 == 0\n",
    "    \n",
    "    def extraire_siren_du_texte(self, texte):\n",
    "        \"\"\"Extraire les SIREN d'un texte avec regex\"\"\"\n",
    "        patterns = [\n",
    "            r'\\b\\d{9}\\b',\n",
    "            r'\\b\\d{3}[\\s-]\\d{3}[\\s-]\\d{3}\\b',\n",
    "            r'\\b\\d{3}\\s+\\d{3}\\s+\\d{3}\\b'\n",
    "        ]\n",
    "        \n",
    "        numeros_siren = []\n",
    "        for pattern in patterns:\n",
    "            for match in re.finditer(pattern, texte):\n",
    "                siren_propre = re.sub(r'\\D', '', match.group())\n",
    "                if len(siren_propre) == 9:\n",
    "                    numeros_siren.append(siren_propre)\n",
    "        \n",
    "        return list(dict.fromkeys(numeros_siren))\n",
    "    \n",
    "    def generer_donnees_entrainement(self, sirens_extraits):\n",
    "        \"\"\"G√©n√©rer des donn√©es d'entra√Ænement √† partir des SIREN extraits\"\"\"\n",
    "        templates = [\n",
    "            \"La soci√©t√© est immatricul√©e sous le num√©ro {siren}\",\n",
    "            \"SIREN : {siren}\",\n",
    "            \"Le SIREN de l'entreprise est {siren}.\",\n",
    "            \"Num√©ro SIREN {siren} valid√©\",\n",
    "            \"RCS Paris {siren}\",\n",
    "            \"Identifiant SIREN: {siren}\",\n",
    "            \"N¬∞ SIREN : {siren}\",\n",
    "            \"Soci√©t√© immatricul√©e {siren}\",\n",
    "            \"SIREN {siren} entreprise\",\n",
    "            \"Num√©ro d'identification {siren}\",\n",
    "            \"Enregistr√©e sous le SIREN {siren}\",\n",
    "            \"Code SIREN {siren}\",\n",
    "            \"Immatriculation SIREN : {siren}\",\n",
    "            \"Le num√©ro SIREN est {siren}\",\n",
    "            \"SIREN de la soci√©t√© : {siren}\"\n",
    "        ]\n",
    "        \n",
    "        nouvelles_donnees = []\n",
    "        \n",
    "        for siren in sirens_extraits:\n",
    "            if self.valider_siren(siren):  # Utiliser seulement les SIREN valides\n",
    "                for template in templates:\n",
    "                    texte = template.format(siren=siren)\n",
    "                    \n",
    "                    # Trouver la position du SIREN dans le texte\n",
    "                    start = texte.find(siren)\n",
    "                    end = start + len(siren)\n",
    "                    \n",
    "                    if start != -1:\n",
    "                        nouvelles_donnees.append((texte, {\"entities\": [(start, end, \"SIREN\")]}))\n",
    "        \n",
    "        return nouvelles_donnees\n",
    "    \n",
    "    def traiter_fichier_json(self, chemin_json):\n",
    "        \"\"\"Traiter un fichier JSON pour extraire les SIREN\"\"\"\n",
    "        try:\n",
    "            with open(chemin_json, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extraire le texte selon la structure du JSON\n",
    "            texte_complet = \"\"\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                # Parcourir r√©cursivement le JSON pour extraire le texte\n",
    "                texte_complet = self._extraire_texte_json(data)\n",
    "            elif isinstance(data, list):\n",
    "                for item in data:\n",
    "                    texte_complet += self._extraire_texte_json(item) + \" \"\n",
    "            \n",
    "            # Extraire les SIREN du texte\n",
    "            sirens_extraits = self.extraire_siren_du_texte(texte_complet)\n",
    "            \n",
    "            return sirens_extraits, texte_complet\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du fichier {chemin_json}: {e}\")\n",
    "            return [], \"\"\n",
    "    \n",
    "    def _extraire_texte_json(self, obj):\n",
    "        \"\"\"Extraire r√©cursivement le texte d'un objet JSON\"\"\"\n",
    "        texte = \"\"\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                texte += self._extraire_texte_json(value) + \" \"\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                texte += self._extraire_texte_json(item) + \" \"\n",
    "        elif isinstance(obj, str):\n",
    "            texte += obj + \" \"\n",
    "        \n",
    "        return texte\n",
    "    \n",
    "    def traiter_dossier_json(self, dossier_json):\n",
    "        \"\"\"Traiter tous les fichiers JSON d'un dossier\"\"\"\n",
    "        dossier_path = Path(dossier_json)\n",
    "        if not dossier_path.exists():\n",
    "            print(f\"Le dossier {dossier_json} n'existe pas.\")\n",
    "            return []\n",
    "        \n",
    "        tous_sirens = []\n",
    "        fichiers_traites = []\n",
    "        \n",
    "        for fichier_json in dossier_path.glob(\"*.json\"):\n",
    "            print(f\"Traitement de {fichier_json.name}...\")\n",
    "            sirens, texte = self.traiter_fichier_json(fichier_json)\n",
    "            \n",
    "            if sirens:\n",
    "                tous_sirens.extend(sirens)\n",
    "                fichiers_traites.append({\n",
    "                    'fichier': fichier_json.name,\n",
    "                    'sirens': sirens,\n",
    "                    'nb_sirens': len(sirens)\n",
    "                })\n",
    "        \n",
    "        print(f\"\\\\nTraitement termin√©:\")\n",
    "        print(f\"- {len(fichiers_traites)} fichiers trait√©s\")\n",
    "        print(f\"- {len(set(tous_sirens))} SIREN uniques extraits\")\n",
    "        \n",
    "        return list(set(tous_sirens))  # Retourner les SIREN uniques\n",
    "    \n",
    "    def creer_dataset_entrainement(self, sirens_extraits=None, nom_fichier=\"train_siren.spacy\"):\n",
    "        \"\"\"Cr√©er un dataset d'entra√Ænement complet\"\"\"\n",
    "        \n",
    "        # Commencer avec les donn√©es de base\n",
    "        train_data = self.train_data_base.copy()\n",
    "        \n",
    "        # Ajouter les donn√©es g√©n√©r√©es √† partir des SIREN de base\n",
    "        for siren in self.liste_siren_base:\n",
    "            train_data.extend(self.generer_donnees_entrainement([siren]))\n",
    "        \n",
    "        # Ajouter les donn√©es des SIREN extraits\n",
    "        if sirens_extraits:\n",
    "            nouvelles_donnees = self.generer_donnees_entrainement(sirens_extraits)\n",
    "            train_data.extend(nouvelles_donnees)\n",
    "        \n",
    "        # M√©langer les donn√©es\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        # Cr√©er le DocBin\n",
    "        nlp = spacy.blank(\"fr\")\n",
    "        doc_bin = DocBin()\n",
    "        \n",
    "        donnees_valides = 0\n",
    "        \n",
    "        for text, annot in train_data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            ents = []\n",
    "            \n",
    "            for start, end, label in annot[\"entities\"]:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if span:\n",
    "                    ents.append(span)\n",
    "            \n",
    "            if ents:  # Ajouter seulement si des entit√©s ont √©t√© trouv√©es\n",
    "                doc.ents = ents\n",
    "                doc_bin.add(doc)\n",
    "                donnees_valides += 1\n",
    "        \n",
    "        # Sauvegarder\n",
    "        chemin_sortie = self.dossier_donnees / nom_fichier\n",
    "        doc_bin.to_disk(chemin_sortie)\n",
    "        \n",
    "        print(f\"Dataset cr√©√© avec {donnees_valides} exemples valides\")\n",
    "        print(f\"Sauvegard√© dans: {chemin_sortie}\")\n",
    "        \n",
    "        return str(chemin_sortie)\n",
    "    \n",
    "    def entrainer_modele(self, chemin_dataset, nom_modele=\"modele_siren\", n_iter=30):\n",
    "        \"\"\"Entra√Æner un mod√®le spaCy pour la reconnaissance des SIREN\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Charger le mod√®le de base\n",
    "            nlp = spacy.load(self.modele_base)\n",
    "            print(f\"Mod√®le de base charg√©: {self.modele_base}\")\n",
    "        except OSError:\n",
    "            print(f\"Mod√®le {self.modele_base} non trouv√©. Utilisation d'un mod√®le vide.\")\n",
    "            nlp = spacy.blank(\"fr\")\n",
    "        \n",
    "        # Ajouter le composant NER s'il n'existe pas\n",
    "        if \"ner\" not in nlp.pipe_names:\n",
    "            ner = nlp.add_pipe(\"ner\")\n",
    "        else:\n",
    "            ner = nlp.get_pipe(\"ner\")\n",
    "        \n",
    "        # Ajouter le label SIREN\n",
    "        ner.add_label(\"SIREN\")\n",
    "        \n",
    "        # Charger les donn√©es d'entra√Ænement\n",
    "        doc_bin = DocBin().from_disk(chemin_dataset)\n",
    "        docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "        \n",
    "        # Cr√©er les exemples d'entra√Ænement\n",
    "        examples = []\n",
    "        for doc in docs:\n",
    "            example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})\n",
    "            examples.append(example)\n",
    "        \n",
    "        print(f\"Entra√Ænement avec {len(examples)} exemples...\")\n",
    "        \n",
    "        # D√©sactiver les autres pipes pendant l'entra√Ænement\n",
    "        unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "        \n",
    "        with nlp.disable_pipes(*unaffected_pipes):\n",
    "            # Commencer l'entra√Ænement\n",
    "            nlp.begin_training()\n",
    "            \n",
    "            for iteration in range(n_iter):\n",
    "                random.shuffle(examples)\n",
    "                losses = {}\n",
    "                \n",
    "                # Entra√Æner par batches\n",
    "                batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
    "                for batch in batches:\n",
    "                    nlp.update(batch, drop=0.5, losses=losses)\n",
    "                \n",
    "                if iteration % 5 == 0:\n",
    "                    print(f\"It√©ration {iteration}: Perte = {losses.get('ner', 0):.3f}\")\n",
    "        \n",
    "        # Sauvegarder le mod√®le\n",
    "        dossier_modele = self.dossier_donnees / nom_modele\n",
    "        nlp.to_disk(dossier_modele)\n",
    "        \n",
    "        print(f\"\\\\nMod√®le entra√Æn√© et sauvegard√© dans: {dossier_modele}\")\n",
    "        return str(dossier_modele)\n",
    "    \n",
    "    def tester_modele(self, chemin_modele, textes_test=None):\n",
    "        \"\"\"Tester le mod√®le entra√Æn√©\"\"\"\n",
    "        try:\n",
    "            nlp = spacy.load(chemin_modele)\n",
    "            \n",
    "            if textes_test is None:\n",
    "                textes_test = [\n",
    "                    \"La soci√©t√© ACME a le SIREN 123456789\",\n",
    "                    \"RCS Paris 987654321\",\n",
    "                    \"Num√©ro SIREN : 741852963\",\n",
    "                    \"L'entreprise est immatricul√©e sous le num√©ro 456789123\"\n",
    "                ]\n",
    "            \n",
    "            print(\"\\\\n=== TEST DU MOD√àLE ===\")\n",
    "            for texte in textes_test:\n",
    "                doc = nlp(texte)\n",
    "                print(f\"\\\\nTexte: {texte}\")\n",
    "                if doc.ents:\n",
    "                    for ent in doc.ents:\n",
    "                        if ent.label_ == \"SIREN\":\n",
    "                            print(f\"  ‚Üí SIREN d√©tect√©: {ent.text} (confiance: {ent._.confidence if hasattr(ent._, 'confidence') else 'N/A'})\")\n",
    "                else:\n",
    "                    print(\"  ‚Üí Aucun SIREN d√©tect√©\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du test: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    # Configuration\n",
    "    trainer = SirenTrainer()\n",
    "    \n",
    "    # √âtape 1: Traiter les fichiers JSON\n",
    "    dossier_json = input(\"Entrez le chemin du dossier contenant les fichiers JSON: \").strip()\n",
    "    if not dossier_json:\n",
    "        dossier_json = \"fichiers_json\"  # Dossier par d√©faut\n",
    "    \n",
    "    print(\"\\\\n=== EXTRACTION DES SIREN DEPUIS LES FICHIERS JSON ===\")\n",
    "    sirens_extraits = trainer.traiter_dossier_json(dossier_json)\n",
    "    \n",
    "    if sirens_extraits:\n",
    "        print(f\"\\\\nSIREN extraits: {sirens_extraits}\")\n",
    "        \n",
    "        # √âtape 2: Cr√©er le dataset d'entra√Ænement\n",
    "        print(\"\\\\n=== CR√âATION DU DATASET D'ENTRA√éNEMENT ===\")\n",
    "        chemin_dataset = trainer.creer_dataset_entrainement(sirens_extraits)\n",
    "        \n",
    "        # √âtape 3: Entra√Æner le mod√®le\n",
    "        print(\"\\\\n=== ENTRA√éNEMENT DU MOD√àLE ===\")\n",
    "        chemin_modele = trainer.entrainer_modele(chemin_dataset)\n",
    "        \n",
    "        # √âtape 4: Tester le mod√®le\n",
    "        print(\"\\\\n=== TEST DU MOD√àLE ===\")\n",
    "        trainer.tester_modele(chemin_modele)\n",
    "        \n",
    "        print(f\"\\\\n‚úÖ Processus termin√© avec succ√®s!\")\n",
    "        print(f\"üìÅ Mod√®le sauvegard√© dans: {chemin_modele}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Aucun SIREN trouv√© dans les fichiers JSON.\")\n",
    "        \n",
    "        # Cr√©er quand m√™me un dataset avec les donn√©es de base\n",
    "        print(\"\\\\nCr√©ation d'un dataset avec les donn√©es de base...\")\n",
    "        chemin_dataset = trainer.creer_dataset_entrainement()\n",
    "        chemin_modele = trainer.entrainer_modele(chemin_dataset)\n",
    "        trainer.tester_modele(chemin_modele)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
